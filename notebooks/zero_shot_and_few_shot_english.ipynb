{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbpJN35cVYAI",
        "outputId": "43441c3f-7266-4368-b6c1-387f62160908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.9.0+cu128 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision==0.24.0+cu128 in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
            "Requirement already satisfied: torchaudio==2.9.0+cu128 in /usr/local/lib/python3.12/dist-packages (2.9.0+cu128)\n",
            "Requirement already satisfied: transformers==5.0.0 in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: accelerate==1.12.0 in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting datasets==4.5.0\n",
            "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate==0.4.3\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: peft==0.18.1 in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
            "Collecting trl==0.27.2\n",
            "  Downloading trl-0.27.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting bitsandbytes==0.49.1\n",
            "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting huggingface-hub==1.3.7\n",
            "  Downloading huggingface_hub-1.3.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: tokenizers==0.22.2 in /usr/local/lib/python3.12/dist-packages (0.22.2)\n",
            "Collecting sqlite-utils==3.38\n",
            "  Downloading sqlite_utils-3.38-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting sqlalchemy==2.0.30\n",
            "  Downloading SQLAlchemy-2.0.30-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0+cu128) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.24.0+cu128) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.24.0+cu128) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0) (2025.11.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==5.0.0) (4.67.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.12.0) (5.9.5)\n",
            "Collecting pyarrow>=21.0.0 (from datasets==4.5.0)\n",
            "  Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.5.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.5.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.5.0) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.5.0) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.5.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets==4.5.0) (0.70.16)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==1.3.7) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==1.3.7) (1.5.4)\n",
            "Collecting sqlite-fts4 (from sqlite-utils==3.38)\n",
            "  Downloading sqlite_fts4-1.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sqlite-utils==3.38) (8.3.1)\n",
            "Collecting click-default-group>=1.2.3 (from sqlite-utils==3.38)\n",
            "  Downloading click_default_group-1.2.4-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from sqlite-utils==3.38) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from sqlite-utils==3.38) (2.9.0.post0)\n",
            "Requirement already satisfied: pluggy in /usr/local/lib/python3.12/dist-packages (from sqlite-utils==3.38) (1.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy==2.0.30) (3.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.5.0) (3.13.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.5.0) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.5.0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.5.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.5.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets==4.5.0) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.5.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.5.0) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0+cu128) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0+cu128) (3.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.5.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.5.0) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->sqlite-utils==3.38) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.5.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.5.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.5.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.5.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.5.0) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.5.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets==4.5.0) (1.22.0)\n",
            "Downloading datasets-4.5.0-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.27.2-py3-none-any.whl (530 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m530.9/530.9 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-1.3.7-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlite_utils-3.38-py3-none-any.whl (68 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-2.0.30-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_default_group-1.2.4-py2.py3-none-any.whl (4.1 kB)\n",
            "Downloading pyarrow-23.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlite_fts4-1.0.3-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: sqlite-fts4, sqlalchemy, pyarrow, click-default-group, sqlite-utils, huggingface-hub, datasets, bitsandbytes, evaluate, trl\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.46\n",
            "    Uninstalling SQLAlchemy-2.0.46:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.46\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 18.1.0\n",
            "    Uninstalling pyarrow-18.1.0:\n",
            "      Successfully uninstalled pyarrow-18.1.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.0\n",
            "    Uninstalling huggingface_hub-1.4.0:\n",
            "      Successfully uninstalled huggingface_hub-1.4.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed bitsandbytes-0.49.1 click-default-group-1.2.4 datasets-4.5.0 evaluate-0.4.3 huggingface-hub-1.3.7 pyarrow-23.0.0 sqlalchemy-2.0.30 sqlite-fts4-1.0.3 sqlite-utils-3.38 trl-0.27.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \\\n",
        "torch==2.9.0+cu128 torchvision==0.24.0+cu128 torchaudio==2.9.0+cu128 \\\n",
        "transformers==5.0.0 accelerate==1.12.0 datasets==4.5.0 evaluate==0.4.3 \\\n",
        "peft==0.18.1 trl==0.27.2 bitsandbytes==0.49.1 \\\n",
        "huggingface-hub==1.3.7 tokenizers==0.22.2 \\\n",
        "sqlite-utils==3.38 sqlalchemy==2.0.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MqwJNvPSju4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd49ebcf-24c2-4def-c6a2-0e9580c29f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate                               1.12.0\n",
            "datasets                                 4.5.0\n",
            "evaluate                                 0.4.3\n",
            "sentence-transformers                    5.2.2\n",
            "sqlalchemy-spanner                       1.17.2\n",
            "sqlite-utils                             3.38\n",
            "tensorflow-datasets                      4.9.9\n",
            "torch                                    2.9.0+cu128\n",
            "torchao                                  0.10.0\n",
            "torchaudio                               2.9.0+cu128\n",
            "torchcodec                               0.8.0+cu128\n",
            "torchdata                                0.11.0\n",
            "torchsummary                             1.5.1\n",
            "torchtune                                0.6.1\n",
            "torchvision                              0.24.0+cu128\n",
            "transformers                             5.0.0\n",
            "vega-datasets                            0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep -E 'transformers|accelerate|datasets|sqlalchemy|sqlite-utils|moz-sql-parser|evaluate|torch|torchvision'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UpEOcyDxEmpr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193c85cf-7e65-4ef7-c9a3-a0c1d9499314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seeds fixed to 42\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python - <<'PY'\n",
        "import random, numpy as np, torch, os\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "print(\"Seeds fixed to\", SEED)\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ggRRHKJJc9RI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve the Hugging Face API key from Colab's secrets manager\n",
        "hf_token = userdata.get('HF_API_KEY')\n",
        "\n",
        "# Log in to Hugging Face Hub using the retrieved token\n",
        "login(hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PRX-zFKVlXg",
        "outputId": "10abf8b4-441c-4c3e-a25a-32157aded238"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/chichewa-text2sql/data\")\n",
        "\n",
        "train_path = DATA_DIR / \"train.json\"\n",
        "dev_path   = DATA_DIR / \"dev.json\"\n",
        "test_path  = DATA_DIR / \"test.json\"\n",
        "\n",
        "print(\"Train exists:\", train_path.exists())\n",
        "print(\"Dev exists:\", dev_path.exists())\n",
        "print(\"Test exists:\", test_path.exists())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPVS8-rmVvOM",
        "outputId": "0929d520-8e69-4f3d-d68d-7a9f4f3e6f8a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train exists: True\n",
            "Dev exists: True\n",
            "Test exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(path: Path):\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "train_data = load_json(train_path)\n",
        "dev_data   = load_json(dev_path)\n",
        "test_data  = load_json(test_path)\n",
        "\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Dev size:   {len(dev_data)}\")\n",
        "print(f\"Test size:  {len(test_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN7wK38bVyaT",
        "outputId": "0a3d155d-7f89-402d-a1a2-ccc2e5bad81a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 280\n",
            "Dev size:   60\n",
            "Test size:  60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M-9xQnq7O17o"
      },
      "outputs": [],
      "source": [
        "# reflect every .sqlite file to create a plain-text schema description\n",
        "\n",
        "import sqlalchemy as sa\n",
        "\n",
        "DB_PATH = Path(\"/content/drive/MyDrive/chichewa-text2sql/data/database/chichewa_text2sql.db\")\n",
        "\n",
        "def get_schema_string() -> str:\n",
        "    \"\"\"Return a compact textual schema for the given database.\"\"\"\n",
        "\n",
        "    engine  = sa.create_engine(f\"sqlite:///{DB_PATH}\")\n",
        "    insp    = sa.inspect(engine)\n",
        "\n",
        "    parts   = []\n",
        "    for tbl in sorted(insp.get_table_names()):\n",
        "        cols = [c[\"name\"] for c in insp.get_columns(tbl)]\n",
        "        parts.append(f\"{tbl}({', '.join(cols)})\")\n",
        "\n",
        "    schema_str = \", \".join(parts)\n",
        "    return schema_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Cax0Tv1HUzt",
        "outputId": "aced26cd-3e1c-4ec5-a029-4c2fd5876601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "commodity_prices(id, add_name, epa_name, district, market, month_name, year, commodity, price, collection_date), food_insecurity(id, district, analyzed_population, time_period, percentage_population, insecurity_level, insecurity_desc_short, insecurity_desc_long), mse_daily(id, counter_id, ticker, trade_date, print_time, company_name, sector, high_price, low_price, bid_price, ask_price, previous_close_price, close_price, volume, dividend_mwk, dividend_yield_pct, earnings_yield_pct, pe_ratio, pbv_ratio, market_cap_mwk_mn, profit_after_tax_mwk_mn, shares_outstanding), population(id, region_name, region_code, admin_status, district_code, ea_number, ea_code, ta_code, ta_name, population_male, population_female, number_households, district_name, total_population), production(id, district, crop, yield, season)\n"
          ]
        }
      ],
      "source": [
        "schema_str = get_schema_string()\n",
        "print(schema_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "def build_prompt(nl_question: str, schema: str) -> str:\n",
        "    return (\n",
        "        \"### Instruction:\\n\"\n",
        "        \"You are an expert SQL developer. Given a database schema and a natural-language question,\\n\"\n",
        "        \"write ONE syntactically correct SQL query that answers the question.\\n\\n\"\n",
        "\n",
        "        f\"### Database Schema:\\n{schema}\\n\\n\"\n",
        "\n",
        "        f\"### Question:\\n{nl_question}\\n\\n\"\n",
        "\n",
        "        \"### SQL:\\n\")\n"
      ],
      "metadata": {
        "id": "P4YrMIg0kVRr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W73kzpDMgxup"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zg2l13lHqWAq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "202566a527cd42b983fb03e9d2b5d5da",
            "4bb8956f35dc44bab2e0278864a0e16a",
            "fd50fa9aacff4a269f37bebb9e2e79ea",
            "996fbb8b41a042a88a943e45f842e94d",
            "b7dfc9e595934b76a962ccf59125db98",
            "0dc2bd1b32bc4a9687c04ba245357dee",
            "cd663b83a4654a49b0bb9c0ea6e77efb",
            "d6e47e2a7c554689bfd0dec9fb407962",
            "6715bc764e494f3f868be5f3d4caab3d",
            "4f4284ad907045b99398a28067cad4ef",
            "29a861ea8dff43e8ad8bffdc0901470b"
          ]
        },
        "outputId": "06bdb708-2e6b-465a-a89e-cbb514282c3b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "202566a527cd42b983fb03e9d2b5d5da"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# 0 ▪ global seed (reproducible shuffles, torch, numpy, random, etc.)\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "import random, numpy as np, torch, os, gc\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# 1 ▪ zero-shot prompt builder (no demos)\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "def build_prompt_zero(nl_question: str, target_schema: str) -> str:\n",
        "    return (\n",
        "        \"### Instruction:\\n\"\n",
        "        \"You are an expert SQL developer. Given the database schema and the \"\n",
        "        \"question, return ONE valid SQL statement — output ONLY the SQL.\\n\\n\"\n",
        "        f\"### Database Schema:\\n{target_schema}\\n\\n\"\n",
        "        f\"### Question:\\n{nl_question}\\n\\n\"\n",
        "        \"### SQL:\\n\"\n",
        "    )\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# 2 ▪ random-5-shot builder\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "NUM_SHOTS = 5\n",
        "\n",
        "# Convert train_data to a Hugging Face Dataset object to use .shuffle() and .select()\n",
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "\n",
        "DEMO_SET = [\n",
        "    {**ex, \"schema_str\": get_schema_string()}\n",
        "    for ex in train_dataset.shuffle(seed=SEED).select(range(NUM_SHOTS))\n",
        "]\n",
        "\n",
        "def build_prompt_random5(nl_question: str, target_schema: str) -> str:\n",
        "    parts = [\"### Instruction:\\nReturn ONE SQL query only, based on the examples provided.\\n\"]\n",
        "    for i, ex in enumerate(DEMO_SET, 1):\n",
        "        parts += [\n",
        "            f\"### Example {i} Schema:\\n{ex['schema_str']}\",\n",
        "            f\"### Example {i} Question:\\n{ex['question_en']}\",\n",
        "            f\"### Example {i} SQL:\\n{ex['sql_statement'].strip()}\",\n",
        "        ]\n",
        "    parts += [\n",
        "        f\"### Database Schema:\\n{target_schema}\",\n",
        "        f\"### Question:\\n{nl_question}\",\n",
        "        \"### SQL:\\n\",\n",
        "    ]\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# 3 ▪ retrieved-5-shot builder (SBERT nearest neighbours)\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "embedder   = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "train_emb  = embedder.encode(train_dataset[\"question_en\"], convert_to_tensor=True)\n",
        "\n",
        "def get_k_shots(nl_question: str, k: int = 5):\n",
        "    q_emb  = embedder.encode(nl_question, convert_to_tensor=True)\n",
        "    hits   = util.semantic_search(q_emb, train_emb, top_k=k)[0]\n",
        "    demos  = []\n",
        "    for h in hits:\n",
        "        ex = train_dataset[int(h[\"corpus_id\"])]\n",
        "        demos.append({\n",
        "            \"question\"   : ex[\"question_en\"],\n",
        "            \"query\"      : ex[\"sql_statement\"],\n",
        "            \"schema_str\" : get_schema_string(),\n",
        "        })\n",
        "    return demos\n",
        "\n",
        "def build_prompt_retrieved5(nl_question: str, target_schema: str) -> str:\n",
        "    demos = get_k_shots(nl_question, 5)\n",
        "    parts = [\"### Instruction:\\nReturn ONE SQL query only, based on the examples provided.\\n\"]\n",
        "    for i, ex in enumerate(demos, 1):\n",
        "        parts += [\n",
        "            f\"### Example {i} Schema:\\n{ex['schema_str']}\",\n",
        "            f\"### Example {i} Question:\\n{ex['question']}\",\n",
        "            f\"### Example {i} SQL:\\n{ex['query'].strip()}\",\n",
        "            \"### End\\n\",\n",
        "        ]\n",
        "    parts += [\n",
        "        f\"### Database Schema:\\n{target_schema}\",\n",
        "        f\"### Question:\\n{nl_question}\",\n",
        "        \"### SQL:\\n\",\n",
        "    ]\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "# 4 ▪ shared generate_sql that accepts a *builder* argument\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "import re\n",
        "\n",
        "def make_generator(builder_fn, tokenizer, model): # Modified to accept tokenizer and model\n",
        "    def _gen(nl_question: str, schema: str) -> str:\n",
        "        prompt = builder_fn(nl_question, schema)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "        model.config.pad_token_id = tokenizer.eos_token_id\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            num_beams=3,\n",
        "            early_stopping=True,\n",
        "            do_sample=False,\n",
        "        )\n",
        "        gen_text = tokenizer.decode(out[0, input_len:], skip_special_tokens=True)\n",
        "        sql = re.split(r\"(###|\\n\\s*\\n|```)\", gen_text, 1)[0]\n",
        "        sql = sql.split(\";\")[0].replace(\"\\n\", \" \").strip()\n",
        "        return sql.split(\";\", 1)[0].strip()\n",
        "    return _gen"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d6xmbQxihU3A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FdGdDMF2HEcg"
      },
      "outputs": [],
      "source": [
        "# choose the prompt mode here ─────────────────────────────────────────────\n",
        "PROMPT_MODE = 0          # 0 = zero-shot, 1 = random-5, 2 = retrieved-5\n",
        "builder = [build_prompt_zero,\n",
        "           build_prompt_random5,\n",
        "           build_prompt_retrieved5][PROMPT_MODE]\n",
        "generate_sql = make_generator(builder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SZmtUIQfonbc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Define the dictionary of models\n",
        "MODELS = {\n",
        "    \"sqlcoder_7b\"        : \"defog/sqlcoder-7b-2\",\n",
        "    \"deepseek_coder_inst\": \"deepseek-ai/deepseek-coder-6.7b-instruct\",\n",
        "    \"llama3_8b_inst\"     : \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"bloomz_7b_mt\"       : \"bigscience/bloomz-7b1-mt\",\n",
        "    \"codellama_7b_inst\"  : \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "}\n",
        "\n",
        "def load_model(model_path: str):\n",
        "    \"\"\"Loads the tokenizer and model from HuggingFace.\"\"\"\n",
        "    print(f\"Loading model: {model_path}...\")\n",
        "    tokenizer  = AutoTokenizer.from_pretrained(model_path)\n",
        "    model      = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    model.eval()\n",
        "    print(f\"Model {model_path} loaded successfully.\")\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "vRGHQP2-DAhE",
        "outputId": "b299f72e-f3c5-4333-ab69-ca10aed5c031"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': 11,\n",
              " 'question_en': 'What was the yield of wheat in nkhotakota?',\n",
              " 'question_ny': 'Ndi tiligu ochuluka bwanj adakololedwa ku Nkhotakota?',\n",
              " 'sql_statement': \"SELECT Yield FROM production WHERE District = 'Nkhotakota' AND Crop = 'Wheat';\",\n",
              " 'sql_result': '[(0.0,)]',\n",
              " 'difficulty_level': 'easy',\n",
              " 'table': 'production'}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "\n",
            "Gold SQL: SELECT Yield FROM production WHERE District = 'Nkhotakota' AND Crop = 'Wheat';\n",
            "Pred SQL: SELECT p.yield FROM production p WHERE p.district = 'nkhotakota' AND p.crop = 'wheat'\n"
          ]
        }
      ],
      "source": [
        "example      = test_data[2]\n",
        "display(example)\n",
        "print(\"==\"*50 + \"\\n\")\n",
        "schema_str   = get_schema_string()\n",
        "predicted_sql= generate_sql(example[\"question_en\"], schema_str)\n",
        "print(\"Gold SQL:\", example[\"sql_statement\"])\n",
        "print(\"Pred SQL:\", predicted_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GKJgHDZanbXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e4aff68-6ce7-4113-b5de-5f05e8699831"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 11,\n",
              " 'question_en': 'What was the yield of wheat in nkhotakota?',\n",
              " 'question_ny': 'Ndi tiligu ochuluka bwanj adakololedwa ku Nkhotakota?',\n",
              " 'sql_statement': \"SELECT Yield FROM production WHERE District = 'Nkhotakota' AND Crop = 'Wheat';\",\n",
              " 'sql_result': '[(0.0,)]',\n",
              " 'difficulty_level': 'easy',\n",
              " 'table': 'production'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "test_data[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QKS8_46KoSi-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "_ws   = re.compile(r\"\\s+\")\n",
        "_comm = re.compile(r\"\\s*,\\s*\")          # blank(s) ↔ comma ↔ blank(s)\n",
        "\n",
        "def exact_match(pred: str, gold: str) -> bool:\n",
        "    def norm(s: str) -> str:\n",
        "        s = s.strip().lower()\n",
        "        s = _ws.sub(\" \", s)             # collapse runs of whitespace\n",
        "        s = _comm.sub(\", \", s)          # canonical “comma␣”\n",
        "        s = s.rstrip(';')                # Remove trailing semicolon\n",
        "        return s\n",
        "    return norm(pred) == norm(gold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1O-8Fw5MxE8w"
      },
      "outputs": [],
      "source": [
        "from sqlglot import parse_one, expressions\n",
        "\n",
        "def flatten_ast(node):\n",
        "    \"\"\"\n",
        "    Recursively collect all node‐type names and literal values as lowercase strings.\n",
        "    \"\"\"\n",
        "    out = set()\n",
        "\n",
        "    def walk(n):\n",
        "        # record the AST node type\n",
        "        out.add(type(n).__name__.lower())\n",
        "\n",
        "        # record any literal (e.g. identifiers, strings, numbers)\n",
        "        if hasattr(n, \"this\") and isinstance(n.this, (str, int, float)):\n",
        "            out.add(str(n.this).lower())\n",
        "\n",
        "        # recurse into child expressions\n",
        "        for arg in n.args.values():\n",
        "            if isinstance(arg, list):\n",
        "                for child in arg:\n",
        "                    if isinstance(child, expressions.Expression):\n",
        "                        walk(child)\n",
        "            elif isinstance(arg, expressions.Expression):\n",
        "                walk(arg)\n",
        "\n",
        "    walk(node)\n",
        "    return out\n",
        "\n",
        "def component_match(pred_sql, gold_sql):\n",
        "    try:\n",
        "        pred_ast = parse_one(pred_sql)\n",
        "        gold_ast = parse_one(gold_sql)\n",
        "    except Exception:\n",
        "        return 0.0\n",
        "\n",
        "    pred_set = flatten_ast(pred_ast)\n",
        "    gold_set = flatten_ast(gold_ast)\n",
        "    if not gold_set:\n",
        "        return 0.0\n",
        "    return len(pred_set & gold_set) / len(gold_set)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9QXKGCbe6KZz"
      },
      "outputs": [],
      "source": [
        "import sqlite3, pandas as pd, numpy as np\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "def run_query(sql: str, db_path: Path):\n",
        "    \"\"\"Return query result as a sorted list of tuples (order-independent).\"\"\"\n",
        "    try:\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            df = pd.read_sql_query(sql, conn)\n",
        "        # sort rows + cols for order-invariant comparison\n",
        "        return tuple(map(tuple, df.sort_index(axis=1).sort_values(list(df.columns)).to_numpy()))\n",
        "    except Exception as e:\n",
        "\n",
        "        return f\"ERROR-{e}\"\n",
        "\n",
        "def execution_accuracy(dataset):\n",
        "    \"\"\"Compute Spider-style Execution Accuracy on the test split\"\"\"\n",
        "    correct = 0\n",
        "    for ex in tqdm(dataset, desc=\"Evaluating\"):\n",
        "\n",
        "        schema  = schema_str\n",
        "\n",
        "        pred_sql= generate_sql(ex[\"question_en\"], schema)\n",
        "\n",
        "        # Use global DB_PATH\n",
        "        gold    = run_query(ex[\"sql_statement\"], DB_PATH)\n",
        "        pred    = run_query(pred_sql, DB_PATH)\n",
        "\n",
        "        if gold == pred:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPEFqKOLwCpE",
        "outputId": "e972dd4e-ad5f-4c3a-aceb-3d26b757d815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gold SQL: SELECT Yield FROM production WHERE District = 'Nkhotakota' AND Crop = 'Wheat';\n",
            "Pred SQL: SELECT p.yield FROM production p WHERE p.district = 'nkhotakota' AND p.crop = 'wheat'\n"
          ]
        }
      ],
      "source": [
        "example      = test_data[2]\n",
        "schema_str   = get_schema_string()\n",
        "predicted_sql= generate_sql(example[\"question_en\"], schema_str)\n",
        "print(\"Gold SQL:\", example[\"sql_statement\"])\n",
        "print(\"Pred SQL:\", predicted_sql)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vap2MOwMRHfU",
        "outputId": "87d18f60-96bf-41e7-b8cb-bf9b5bb6ea01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "exact_match(predicted_sql, example[\"sql_statement\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRxD89qlRNwl",
        "outputId": "d4a0a3fa-e109-4c34-bfa1-83253265d7bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "component_match(predicted_sql, example[\"sql_statement\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import Dataset\n",
        "\n",
        "# # Convert test_data to a Hugging Face Dataset object\n",
        "# test_dataset = Dataset.from_list(test_data)\n",
        "\n",
        "# # Calculate and print execution accuracy\n",
        "# accuracy = execution_accuracy(test_dataset)\n",
        "# print(f\"Execution Accuracy on test data: {accuracy}\")"
      ],
      "metadata": {
        "id": "RXdowVBHCTgI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msBLv_QXrlfs"
      },
      "source": [
        "# zero shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2kGHyD4WCk_",
        "outputId": "68ff5a4c-4e97-4261-8a54-9e65cb5b2433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 60/60 [02:08<00:00,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Exact Match        : 0.0\n",
            "Component Match    : 0.8517431748391501\n",
            "Execution Accuracy : 0.13333333333333333\n",
            "Avg. Latency  (s)  : 2.1357557217333465\n",
            "95% Latency  (s)   : 3.4427179628499855\n",
            "GPU Mem Peak       : 14.750436864 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import time, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from transformers import logging\n",
        "import random\n",
        "from datasets import Dataset # Import Dataset here\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# choose the prompt mode here ─────────────────────────────────────────────\n",
        "PROMPT_MODE = 0          # 0 = zero-shot, 1 = random-5, 2 = retrieved-5\n",
        "builder = [build_prompt_zero,\n",
        "           build_prompt_random5,\n",
        "           build_prompt_retrieved5][PROMPT_MODE]\n",
        "generate_sql = make_generator(builder)\n",
        "\n",
        "# 1)\n",
        "# Use test_data as the evaluation dataset\n",
        "# Ensure it's converted to a Hugging Face Dataset for shuffle/select methods\n",
        "test_dataset = Dataset.from_list(test_data)\n",
        "num_samples = len(test_dataset)\n",
        "sample_test  = test_dataset.shuffle(seed=42).select(range(num_samples))\n",
        "\n",
        "\n",
        "# 2) containers\n",
        "em_scores, cm_scores, ex_scores, times = [], [], [], []\n",
        "\n",
        "# 3) start fresh CUDA-peak tracking\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# 4) main loop\n",
        "for ex in tqdm(sample_test, desc=\"Evaluating\"):\n",
        "    # db_id = ex[\"db_id\"] # Not used since get_schema_string does not take db_id\n",
        "    # schema = get_schema_string(db_id) # Incorrect, get_schema_string takes no args\n",
        "    schema = schema_str # Use the globally defined schema_str\n",
        "\n",
        "    t0      = time.perf_counter()\n",
        "    pred_sql= generate_sql(ex[\"question_en\"], schema)\n",
        "    times.append(time.perf_counter() - t0)\n",
        "\n",
        "    gold_sql= ex[\"sql_statement\"]\n",
        "\n",
        "    # exact + component match\n",
        "    em_scores.append( float(exact_match(pred_sql, gold_sql)) )\n",
        "    cm_scores.append( component_match(pred_sql, gold_sql) )\n",
        "\n",
        "    # execution accuracy\n",
        "    gold_res= run_query(gold_sql,  DB_PATH)\n",
        "    pred_res= run_query(pred_sql, DB_PATH)\n",
        "    ex_scores.append( int(gold_res == pred_res) )\n",
        "\n",
        "# 5) aggregate & report\n",
        "print(\"\\n\\nExact Match        :\", np.mean(em_scores))\n",
        "print(\"Component Match    :\", np.mean(cm_scores))\n",
        "print(\"Execution Accuracy :\", np.mean(ex_scores))\n",
        "print(\"Avg. Latency  (s)  :\", np.mean(times))\n",
        "print(\"95% Latency  (s)   :\", np.percentile(times, 95))\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Mem Peak       :\", torch.cuda.max_memory_allocated() / 1e9, \"GB\")\n",
        "else:\n",
        "    print(\"GPU Mem Peak       : N/A (CPU)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywNC088rruEc"
      },
      "source": [
        "# Few-shot (Random Sampling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W7YhedmX_Bq",
        "outputId": "174d12fb-9e05-410b-cae2-ef40558bffa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 60/60 [02:27<00:00,  2.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Exact Match        : 0.0\n",
            "Component Match    : 0.9123081811262926\n",
            "Execution Accuracy : 0.45\n",
            "Avg. Latency  (s)  : 2.446125197216664\n",
            "95% Latency  (s)   : 3.568981374050014\n",
            "GPU Mem Peak       : 17.903938048 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on a fixed 20% Spider dev sample\n",
        "import time, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from transformers import logging\n",
        "import random\n",
        "from datasets import Dataset # Import Dataset\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# choose the prompt mode here ─────────────────────────────────────────────\n",
        "PROMPT_MODE = 1          # 0 = zero-shot, 1 = random-5, 2 = retrieved-5\n",
        "builder = [build_prompt_zero,\n",
        "           build_prompt_random5,\n",
        "           build_prompt_retrieved5][PROMPT_MODE]\n",
        "generate_sql = make_generator(builder)\n",
        "\n",
        "# 1)\n",
        "# Use test_data for evaluation\n",
        "test_dataset = Dataset.from_list(test_data)\n",
        "num_samples = len(test_dataset)\n",
        "sample_test  = test_dataset.shuffle(seed=42).select(range(num_samples))\n",
        "\n",
        "\n",
        "# 2) containers\n",
        "em_scores, cm_scores, ex_scores, times = [], [], [], []\n",
        "\n",
        "# 3) start fresh CUDA-peak tracking\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# 4) main loop\n",
        "for ex in tqdm(sample_test, desc=\"Evaluating\"):\n",
        "    # Use global schema_str\n",
        "    schema  = schema_str\n",
        "\n",
        "    t0      = time.perf_counter()\n",
        "    pred_sql= generate_sql(ex[\"question_en\"], schema) # Corrected to question_en\n",
        "    times.append(time.perf_counter() - t0)\n",
        "\n",
        "    gold_sql= ex[\"sql_statement\"] # Corrected to sql_statement\n",
        "\n",
        "    # exact + component match\n",
        "    em_scores.append( float(exact_match(pred_sql, gold_sql)) )\n",
        "    cm_scores.append( component_match(pred_sql, gold_sql) )\n",
        "\n",
        "    # execution accuracy\n",
        "    gold_res= run_query(gold_sql,  DB_PATH) # Use global DB_PATH\n",
        "    pred_res= run_query(pred_sql, DB_PATH) # Use global DB_PATH\n",
        "    ex_scores.append( int(gold_res == pred_res) )\n",
        "\n",
        "# 5) aggregate & report\n",
        "print(\"\\n\\nExact Match        :\", np.mean(em_scores))\n",
        "print(\"Component Match    :\", np.mean(cm_scores))\n",
        "print(\"Execution Accuracy :\", np.mean(ex_scores))\n",
        "print(\"Avg. Latency  (s)  :\", np.mean(times))\n",
        "print(\"95% Latency  (s)   :\", np.percentile(times, 95))\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Mem Peak       :\", torch.cuda.max_memory_allocated() / 1e9, \"GB\")\n",
        "else:\n",
        "    print(\"GPU Mem Peak       : N/A (CPU)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsOy8FVMr20G"
      },
      "source": [
        "# Few-shot (all-MiniLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMwt8sCir3WA",
        "outputId": "d46f8e4b-c2b6-4d73-fa26-525cdd65b0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 60/60 [02:32<00:00,  2.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Exact Match        : 0.0\n",
            "Component Match    : 0.9478143041169357\n",
            "Execution Accuracy : 0.6333333333333333\n",
            "Avg. Latency  (s)  : 2.519987743216677\n",
            "95% Latency  (s)   : 4.405547950499896\n",
            "GPU Mem Peak       : 18.465375232 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation on a fixed 20% Spider dev sample\n",
        "import time, torch, numpy as np\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from transformers import logging\n",
        "import random\n",
        "from datasets import Dataset\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# choose the prompt mode here ─────────────────────────────────────────────\n",
        "PROMPT_MODE = 2          # 0 = zero-shot, 1 = random-5, 2 = retrieved-5\n",
        "builder = [build_prompt_zero,\n",
        "           build_prompt_random5,\n",
        "           build_prompt_retrieved5][PROMPT_MODE]\n",
        "generate_sql = make_generator(builder)\n",
        "\n",
        "# 1)\n",
        "# Use test_data for evaluation\n",
        "test_dataset = Dataset.from_list(test_data)\n",
        "num_samples = len(test_dataset)\n",
        "sample_test  = test_dataset.shuffle(seed=42).select(range(num_samples))\n",
        "\n",
        "\n",
        "# 2) containers\n",
        "em_scores, cm_scores, ex_scores, times = [], [], [], []\n",
        "\n",
        "# 3) start fresh CUDA-peak tracking\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# 4) main loop\n",
        "for ex in tqdm(sample_test, desc=\"Evaluating\"):\n",
        "    # Use global schema_str\n",
        "    schema  = schema_str\n",
        "\n",
        "    t0      = time.perf_counter()\n",
        "    pred_sql= generate_sql(ex[\"question_en\"], schema)\n",
        "    times.append(time.perf_counter() - t0)\n",
        "\n",
        "    gold_sql= ex[\"sql_statement\"]\n",
        "\n",
        "    # exact + component match\n",
        "    em_scores.append( float(exact_match(pred_sql, gold_sql)) )\n",
        "    cm_scores.append( component_match(pred_sql, gold_sql) )\n",
        "\n",
        "    # execution accuracy\n",
        "    gold_res= run_query(gold_sql,  DB_PATH) # Use global DB_PATH\n",
        "    pred_res= run_query(pred_sql, DB_PATH) # Use global DB_PATH\n",
        "    ex_scores.append( int(gold_res == pred_res) )\n",
        "\n",
        "# 5) aggregate & report\n",
        "print(\"\\n\\nExact Match        :\", np.mean(em_scores))\n",
        "print(\"Component Match    :\", np.mean(cm_scores))\n",
        "print(\"Execution Accuracy :\", np.mean(ex_scores))\n",
        "print(\"Avg. Latency  (s)  :\", np.mean(times))\n",
        "print(\"95% Latency  (s)   :\", np.percentile(times, 95))\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Mem Peak       :\", torch.cuda.max_memory_allocated() / 1e9, \"GB\")\n",
        "else:\n",
        "    print(\"GPU Mem Peak       : N/A (CPU)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "202566a527cd42b983fb03e9d2b5d5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4bb8956f35dc44bab2e0278864a0e16a",
              "IPY_MODEL_fd50fa9aacff4a269f37bebb9e2e79ea",
              "IPY_MODEL_996fbb8b41a042a88a943e45f842e94d"
            ],
            "layout": "IPY_MODEL_b7dfc9e595934b76a962ccf59125db98"
          }
        },
        "4bb8956f35dc44bab2e0278864a0e16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc2bd1b32bc4a9687c04ba245357dee",
            "placeholder": "​",
            "style": "IPY_MODEL_cd663b83a4654a49b0bb9c0ea6e77efb",
            "value": "Loading weights: 100%"
          }
        },
        "fd50fa9aacff4a269f37bebb9e2e79ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6e47e2a7c554689bfd0dec9fb407962",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6715bc764e494f3f868be5f3d4caab3d",
            "value": 199
          }
        },
        "996fbb8b41a042a88a943e45f842e94d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f4284ad907045b99398a28067cad4ef",
            "placeholder": "​",
            "style": "IPY_MODEL_29a861ea8dff43e8ad8bffdc0901470b",
            "value": " 199/199 [00:00&lt;00:00, 974.66it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "b7dfc9e595934b76a962ccf59125db98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dc2bd1b32bc4a9687c04ba245357dee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd663b83a4654a49b0bb9c0ea6e77efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6e47e2a7c554689bfd0dec9fb407962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6715bc764e494f3f868be5f3d4caab3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f4284ad907045b99398a28067cad4ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29a861ea8dff43e8ad8bffdc0901470b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}